{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# üìÑ Complete Resume Parser with NER & Entity Linking\n",
        "# Objective: Extract entities from resumes using spaCy and link them to Wikidata"
      ],
      "metadata": {
        "id": "mHWI-Jd6noSY"
      },
      "id": "mHWI-Jd6noSY",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import PyPDF2\n",
        "from io import StringIO\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "68GlQECSnrjH"
      },
      "id": "68GlQECSnrjH",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 1: DATA PREPARATION\n",
        "# ================================\n",
        "\n",
        "class ResumeProcessor:\n",
        "    def __init__(self, model_name: str = \"en_core_web_sm\"):\n",
        "        \"\"\"Initialize the resume processor with spaCy model\"\"\"\n",
        "        try:\n",
        "            self.nlp = spacy.load(model_name)\n",
        "        except OSError:\n",
        "            print(f\"Model {model_name} not found. Please install it using:\")\n",
        "            print(f\"python -m spacy download {model_name}\")\n",
        "            raise\n",
        "\n",
        "        # Add custom patterns for skills and education\n",
        "        self._add_custom_patterns()\n",
        "\n",
        "    def _add_custom_patterns(self):\n",
        "        \"\"\"Add custom patterns for better entity recognition\"\"\"\n",
        "        from spacy.matcher import Matcher\n",
        "\n",
        "        self.matcher = Matcher(self.nlp.vocab)\n",
        "\n",
        "        # Skills patterns - each pattern must be a list of token dictionaries\n",
        "        skills_patterns = [\n",
        "            [{\"LOWER\": {\"IN\": [\"python\", \"java\", \"javascript\", \"c++\", \"sql\", \"html\", \"css\"]}}],\n",
        "            [{\"LOWER\": \"machine\"}, {\"LOWER\": \"learning\"}],\n",
        "            [{\"LOWER\": \"data\"}, {\"LOWER\": \"science\"}],\n",
        "            [{\"LOWER\": \"artificial\"}, {\"LOWER\": \"intelligence\"}],\n",
        "            [{\"LOWER\": \"deep\"}, {\"LOWER\": \"learning\"}],\n",
        "            [{\"LOWER\": \"natural\"}, {\"LOWER\": \"language\"}, {\"LOWER\": \"processing\"}],\n",
        "            [{\"LOWER\": {\"IN\": [\"tensorflow\", \"pytorch\", \"keras\", \"pandas\", \"numpy\", \"scikit-learn\"]}}],\n",
        "            [{\"LOWER\": {\"IN\": [\"aws\", \"azure\", \"gcp\", \"docker\", \"kubernetes\"]}}]\n",
        "        ]\n",
        "\n",
        "        # Education patterns\n",
        "        education_patterns = [\n",
        "            [{\"LOWER\": {\"IN\": [\"bachelor\", \"master\", \"phd\", \"doctorate\"]}},\n",
        "             {\"LOWER\": \"of\"}, {\"LOWER\": {\"IN\": [\"science\", \"arts\", \"engineering\"]}}],\n",
        "            [{\"LOWER\": {\"IN\": [\"b.s.\", \"m.s.\", \"ph.d.\", \"b.a.\", \"m.a.\", \"mba\"]}}],\n",
        "            [{\"LOWER\": {\"IN\": [\"bachelor's\", \"master's\", \"doctorate\"]}}]\n",
        "        ]\n",
        "\n",
        "        self.matcher.add(\"SKILL\", skills_patterns)\n",
        "        self.matcher.add(\"EDUCATION\", education_patterns)\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remove special characters but keep basic punctuation\n",
        "        text = re.sub(r'[^\\w\\s\\-\\.,;:()\\[\\]]', '', text)\n",
        "        # Remove extra newlines\n",
        "        text = re.sub(r'\\n+', '\\n', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_from_pdf(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF file\"\"\"\n",
        "        if not PDF_SUPPORT:\n",
        "            print(\"PDF support not available. Please install PyPDF2: pip install PyPDF2\")\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text()\n",
        "                return self.clean_text(text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading PDF: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_from_text(self, text_path: str) -> str:\n",
        "        \"\"\"Extract text from text file\"\"\"\n",
        "        try:\n",
        "            with open(text_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "                return self.clean_text(text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading text file: {e}\")\n",
        "            return \"\""
      ],
      "metadata": {
        "id": "BaMoreVsnt1_"
      },
      "id": "BaMoreVsnt1_",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 2: NAMED ENTITY RECOGNITION\n",
        "# ================================\n",
        "\n",
        "class EntityExtractor:\n",
        "    def __init__(self, nlp_model):\n",
        "        self.nlp = nlp_model\n",
        "\n",
        "        # Define relevant entity types\n",
        "        self.relevant_entities = {\n",
        "            \"PERSON\": \"Person names\",\n",
        "            \"ORG\": \"Organizations, companies, universities\",\n",
        "            \"GPE\": \"Countries, cities, states\",\n",
        "            \"PRODUCT\": \"Software, tools, products\",\n",
        "            \"WORK_OF_ART\": \"Publications, projects\",\n",
        "            \"DATE\": \"Dates, years\",\n",
        "            \"CARDINAL\": \"Numbers, percentages\",\n",
        "            \"MONEY\": \"Salaries, financial amounts\"\n",
        "        }\n",
        "\n",
        "        # Skills keywords (expanded list)\n",
        "        self.skill_keywords = {\n",
        "            'programming': ['python', 'java', 'javascript', 'c++', 'c#', 'php', 'ruby', 'go', 'rust', 'kotlin'],\n",
        "            'databases': ['mysql', 'postgresql', 'mongodb', 'oracle', 'sqlite', 'redis'],\n",
        "            'frameworks': ['django', 'flask', 'react', 'angular', 'vue', 'spring', 'laravel'],\n",
        "            'ml_ai': ['machine learning', 'deep learning', 'tensorflow', 'pytorch', 'keras', 'scikit-learn'],\n",
        "            'cloud': ['aws', 'azure', 'gcp', 'docker', 'kubernetes', 'terraform'],\n",
        "            'tools': ['git', 'jenkins', 'jira', 'confluence', 'tableau', 'power bi']\n",
        "        }\n",
        "\n",
        "        # Education keywords\n",
        "        self.education_keywords = [\n",
        "            'bachelor', 'master', 'phd', 'doctorate', 'diploma', 'certificate',\n",
        "            'b.s.', 'm.s.', 'ph.d.', 'b.a.', 'm.a.', 'mba', 'b.tech', 'm.tech'\n",
        "        ]\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict:\n",
        "        \"\"\"Extract all types of entities from text\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Extract standard NER entities with better filtering\n",
        "        ner_entities = []\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in self.relevant_entities:\n",
        "                # Filter out common false positives\n",
        "                entity_text = ent.text.strip()\n",
        "                if len(entity_text) > 1 and \\\n",
        "                    not entity_text.isdigit() and \\\n",
        "                    not re.match(r'^\\d{4}$', entity_text):  # Avoid years as standalone entities\n",
        "                    ner_entities.append({\n",
        "                        'text': entity_text,\n",
        "                        'label': ent.label_,\n",
        "                        'description': self.relevant_entities[ent.label_],\n",
        "                        'start': ent.start_char,\n",
        "                        'end': ent.end_char,\n",
        "                        'confidence': 1.0  # spaCy doesn't provide confidence scores by default\n",
        "                    })\n",
        "\n",
        "        # Extract skills using keyword matching\n",
        "        skills = self._extract_skills(text)\n",
        "\n",
        "        # Extract education information\n",
        "        education = self._extract_education(text)\n",
        "\n",
        "        # Extract contact information\n",
        "        contact_info = self._extract_contact_info(text)\n",
        "\n",
        "        return {\n",
        "            'ner_entities': ner_entities,\n",
        "            'skills': skills,\n",
        "            'education': education,\n",
        "            'contact_info': contact_info,\n",
        "            'raw_text': text\n",
        "        }\n",
        "\n",
        "    def _extract_skills(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Extract technical skills from text\"\"\"\n",
        "        skills = []\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for category, skill_list in self.skill_keywords.items():\n",
        "            for skill in skill_list:\n",
        "                if skill in text_lower:\n",
        "                    skills.append({\n",
        "                        'skill': skill.title(),\n",
        "                        'category': category,\n",
        "                        'context': self._get_context(text, skill)\n",
        "                    })\n",
        "\n",
        "        return skills\n",
        "\n",
        "    def _extract_education(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Extract education information\"\"\"\n",
        "        education = []\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Look for education patterns\n",
        "        for sent in doc.sents:\n",
        "            sent_text = sent.text.lower()\n",
        "            for edu_keyword in self.education_keywords:\n",
        "                if edu_keyword in sent_text:\n",
        "                    education.append({\n",
        "                        'degree': self._extract_degree(sent.text),\n",
        "                        'field': self._extract_field(sent.text),\n",
        "                        'institution': self._extract_institution(sent.text),\n",
        "                        'context': sent.text\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "        return education\n",
        "\n",
        "    def _extract_contact_info(self, text: str) -> Dict:\n",
        "        \"\"\"Extract contact information\"\"\"\n",
        "        contact = {}\n",
        "\n",
        "        # Email pattern - more robust\n",
        "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "        emails = re.findall(email_pattern, text)\n",
        "        if emails:\n",
        "            contact['email'] = emails[0]\n",
        "\n",
        "        # Phone pattern - improved to handle various formats\n",
        "        phone_patterns = [\n",
        "            r'\\(\\d{3}\\)\\s?\\d{3}[-.]?\\d{4}',  # (555) 123-4567\n",
        "            r'\\d{3}[-.]?\\d{3}[-.]?\\d{4}',    # 555-123-4567 or 555.123.4567\n",
        "            r'\\+?\\d{1,3}[-.\\s]?\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # +1-555-123-4567\n",
        "            r'\\+?\\d{1,3}[-.\\s]?\\(\\d{3}\\)[-.\\s]?\\d{3}[-.\\s]?\\d{4}'  # +1-(555)-123-4567\n",
        "        ]\n",
        "\n",
        "        for pattern in phone_patterns:\n",
        "            phones = re.findall(pattern, text)\n",
        "            if phones:\n",
        "                # Clean up the phone number\n",
        "                phone = re.sub(r'[^\\d+()]', '', phones[0])\n",
        "                if len(phone) >= 10:  # Valid phone should have at least 10 digits\n",
        "                    contact['phone'] = phones[0]\n",
        "                    break\n",
        "\n",
        "        # LinkedIn profile\n",
        "        linkedin_pattern = r'linkedin\\.com/in/[\\w\\-]+'\n",
        "        linkedin = re.findall(linkedin_pattern, text, re.IGNORECASE)\n",
        "        if linkedin:\n",
        "            contact['linkedin'] = linkedin[0]\n",
        "\n",
        "        return contact\n",
        "\n",
        "    def _get_context(self, text: str, keyword: str, window: int = 50) -> str:\n",
        "        \"\"\"Get context around a keyword\"\"\"\n",
        "        index = text.lower().find(keyword)\n",
        "        if index != -1:\n",
        "            start = max(0, index - window)\n",
        "            end = min(len(text), index + len(keyword) + window)\n",
        "            return text[start:end].strip()\n",
        "        return \"\"\n",
        "\n",
        "    def _extract_degree(self, text: str) -> str:\n",
        "        \"\"\"Extract degree from education text\"\"\"\n",
        "        degree_patterns = [\n",
        "            r'(bachelor|master|phd|doctorate|diploma|certificate)[\\'s]?\\s+of\\s+\\w+',\n",
        "            r'(b\\.s\\.|m\\.s\\.|ph\\.d\\.|b\\.a\\.|m\\.a\\.|mba|b\\.tech|m\\.tech)',\n",
        "        ]\n",
        "\n",
        "        for pattern in degree_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(0)\n",
        "        return \"\"\n",
        "\n",
        "    def _extract_field(self, text: str) -> str:\n",
        "        \"\"\"Extract field of study from education text\"\"\"\n",
        "        fields = ['computer science', 'engineering', 'mathematics', 'physics',\n",
        "                 'business', 'economics', 'psychology', 'biology', 'chemistry']\n",
        "\n",
        "        text_lower = text.lower()\n",
        "        for field in fields:\n",
        "            if field in text_lower:\n",
        "                return field.title()\n",
        "        return \"\"\n",
        "\n",
        "    def _extract_institution(self, text: str) -> str:\n",
        "        \"\"\"Extract institution name from education text\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"ORG\":\n",
        "                return ent.text\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "ezTTDT6Mn8bH"
      },
      "id": "ezTTDT6Mn8bH",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 3: ENTITY LINKING\n",
        "# ================================\n",
        "\n",
        "class EntityLinker:\n",
        "    def __init__(self):\n",
        "        self.wikidata_url = \"https://www.wikidata.org/w/api.php\"\n",
        "        self.dbpedia_url = \"https://lookup.dbpedia.org/api/search\"\n",
        "        self.cache = {}  # Simple caching to avoid repeated API calls\n",
        "\n",
        "    def link_to_wikidata(self, entity: str, limit: int = 3) -> List[Dict]:\n",
        "        \"\"\"Link entity to Wikidata\"\"\"\n",
        "        if entity in self.cache:\n",
        "            return self.cache[entity]\n",
        "\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'language': 'en',\n",
        "            'format': 'json',\n",
        "            'search': entity,\n",
        "            'limit': limit\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.wikidata_url, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = []\n",
        "            if 'search' in data:\n",
        "                for item in data['search']:\n",
        "                    result = {\n",
        "                        'original_entity': entity,\n",
        "                        'wikidata_id': item.get('id', ''),\n",
        "                        'label': item.get('label', ''),\n",
        "                        'description': item.get('description', ''),\n",
        "                        'uri': item.get('concepturi', ''),\n",
        "                        'match_score': self._calculate_match_score(entity, item.get('label', '')),\n",
        "                        'source': 'wikidata'\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "            self.cache[entity] = results\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error linking {entity} to Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def link_to_dbpedia(self, entity: str, limit: int = 3) -> List[Dict]:\n",
        "        \"\"\"Link entity to DBpedia\"\"\"\n",
        "        params = {\n",
        "            'query': entity,\n",
        "            'format': 'json',\n",
        "            'maxResults': limit\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(self.dbpedia_url, params=params, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = []\n",
        "            if 'docs' in data:\n",
        "                for item in data['docs']:\n",
        "                    result = {\n",
        "                        'original_entity': entity,\n",
        "                        'dbpedia_uri': item.get('resource', [''])[0] if item.get('resource') else '',\n",
        "                        'label': item.get('label', [''])[0] if item.get('label') else '',\n",
        "                        'description': item.get('comment', [''])[0] if item.get('comment') else '',\n",
        "                        'categories': item.get('category', []),\n",
        "                        'match_score': self._calculate_match_score(entity,\n",
        "                                     item.get('label', [''])[0] if item.get('label') else ''),\n",
        "                        'source': 'dbpedia'\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error linking {entity} to DBpedia: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _calculate_match_score(self, original: str, candidate: str) -> float:\n",
        "        \"\"\"Calculate similarity score between original and candidate\"\"\"\n",
        "        if not original or not candidate:\n",
        "            return 0.0\n",
        "\n",
        "        original = original.lower().strip()\n",
        "        candidate = candidate.lower().strip()\n",
        "\n",
        "        if original == candidate:\n",
        "            return 1.0\n",
        "        elif original in candidate or candidate in original:\n",
        "            return 0.8\n",
        "        else:\n",
        "            # Simple Jaccard similarity\n",
        "            set1 = set(original.split())\n",
        "            set2 = set(candidate.split())\n",
        "            intersection = len(set1.intersection(set2))\n",
        "            union = len(set1.union(set2))\n",
        "            return intersection / union if union > 0 else 0.0"
      ],
      "metadata": {
        "id": "u_0jv5GUo6O-"
      },
      "id": "u_0jv5GUo6O-",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 4: MAIN RESUME PARSER CLASS\n",
        "# ================================\n",
        "\n",
        "class ResumeParser:\n",
        "    def __init__(self, model_name: str = \"en_core_web_sm\"):\n",
        "        \"\"\"Initialize the complete resume parser\"\"\"\n",
        "        self.processor = ResumeProcessor(model_name)\n",
        "        self.extractor = EntityExtractor(self.processor.nlp)\n",
        "        self.linker = EntityLinker()\n",
        "\n",
        "    def parse_resume(self, file_path: str = None, text: str = None) -> Dict:\n",
        "        \"\"\"Parse a resume and return extracted and linked entities\"\"\"\n",
        "\n",
        "        # Step 1: Get text content\n",
        "        if file_path:\n",
        "            if file_path.lower().endswith('.pdf'):\n",
        "                if not PDF_SUPPORT:\n",
        "                    return {\"error\": \"PDF support not available. Please install PyPDF2 or provide text file.\"}\n",
        "                resume_text = self.processor.extract_from_pdf(file_path)\n",
        "            else:\n",
        "                resume_text = self.processor.extract_from_text(file_path)\n",
        "        elif text:\n",
        "            resume_text = self.processor.clean_text(text)\n",
        "        else:\n",
        "            raise ValueError(\"Either file_path or text must be provided\")\n",
        "\n",
        "        if not resume_text:\n",
        "            return {\"error\": \"Could not extract text from the provided source\"}\n",
        "\n",
        "        # Step 2: Extract entities\n",
        "        extracted_data = self.extractor.extract_entities(resume_text)\n",
        "\n",
        "        # Step 3: Link entities to knowledge bases\n",
        "        linked_results = self._link_all_entities(extracted_data)\n",
        "\n",
        "        # Step 4: Compile final results\n",
        "        final_results = {\n",
        "            'resume_text': resume_text[:500] + \"...\" if len(resume_text) > 500 else resume_text,\n",
        "            'extracted_entities': extracted_data,\n",
        "            'linked_entities': linked_results,\n",
        "            'summary': self._generate_summary(extracted_data, linked_results)\n",
        "        }\n",
        "\n",
        "        return final_results\n",
        "\n",
        "    def _link_all_entities(self, extracted_data: Dict) -> Dict:\n",
        "        \"\"\"Link all extracted entities to knowledge bases\"\"\"\n",
        "        linked_results = {\n",
        "            'wikidata_links': [],\n",
        "            'dbpedia_links': []\n",
        "        }\n",
        "\n",
        "        # Link NER entities - only companies, universities, and important organizations\n",
        "        entities_to_link = []\n",
        "        for entity in extracted_data['ner_entities']:\n",
        "            if (entity['label'] in ['ORG', 'GPE'] and\n",
        "                len(entity['text']) > 2 and\n",
        "                entity['text'].lower() not in ['sql', 'git']):  # Filter out common tech terms\n",
        "                entities_to_link.append(entity['text'])\n",
        "\n",
        "        # Link education institutions\n",
        "        for edu in extracted_data['education']:\n",
        "            if edu['institution'] and len(edu['institution']) > 3:\n",
        "                entities_to_link.append(edu['institution'])\n",
        "\n",
        "        # Link only major skills/technologies (avoid common false positives)\n",
        "        major_skills = ['Python', 'Java', 'JavaScript', 'TensorFlow', 'PyTorch', 'Docker', 'Kubernetes', 'AWS', 'Azure']\n",
        "        for skill in extracted_data['skills']:\n",
        "            if skill['skill'] in major_skills:\n",
        "                entities_to_link.append(skill['skill'])\n",
        "\n",
        "        # Remove duplicates and limit to prevent API overuse\n",
        "        entities_to_link = list(set(entities_to_link))[:5]\n",
        "\n",
        "        # Perform linking with better rate limiting\n",
        "        for i, entity in enumerate(entities_to_link[:5]):  # Reduced limit to prevent API overuse\n",
        "            # Link to Wikidata with longer delays\n",
        "            wikidata_results = self.linker.link_to_wikidata(entity)\n",
        "            linked_results['wikidata_links'].extend(wikidata_results)\n",
        "\n",
        "            # Increased rate limiting to avoid 429 errors\n",
        "            if i < len(entities_to_link[:5]) - 1:  # Don't sleep after last request\n",
        "                time.sleep(2.0)  # Increased from 0.1 to 2.0 seconds\n",
        "\n",
        "        return linked_results\n",
        "\n",
        "    def _generate_summary(self, extracted_data: Dict, linked_results: Dict) -> Dict:\n",
        "        \"\"\"Generate a summary of the parsing results\"\"\"\n",
        "        return {\n",
        "            'total_entities': len(extracted_data['ner_entities']),\n",
        "            'skills_found': len(extracted_data['skills']),\n",
        "            'education_entries': len(extracted_data['education']),\n",
        "            'contact_fields': len(extracted_data['contact_info']),\n",
        "            'wikidata_links': len(linked_results['wikidata_links']),\n",
        "            'dbpedia_links': len(linked_results['dbpedia_links'])\n",
        "        }\n",
        "\n",
        "    def export_to_dataframe(self, results: Dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Export results to pandas DataFrames\"\"\"\n",
        "\n",
        "        # Entities DataFrame\n",
        "        entities_data = []\n",
        "        for entity in results['extracted_entities']['ner_entities']:\n",
        "            entities_data.append({\n",
        "                'Entity': entity['text'],\n",
        "                'Type': entity['label'],\n",
        "                'Description': entity['description'],\n",
        "                'Confidence': entity['confidence']\n",
        "            })\n",
        "\n",
        "        # Skills DataFrame\n",
        "        for skill in results['extracted_entities']['skills']:\n",
        "            entities_data.append({\n",
        "                'Entity': skill['skill'],\n",
        "                'Type': 'SKILL',\n",
        "                'Description': f\"Category: {skill['category']}\",\n",
        "                'Confidence': 0.9\n",
        "            })\n",
        "\n",
        "        entities_df = pd.DataFrame(entities_data)\n",
        "\n",
        "        # Linked entities DataFrame\n",
        "        linked_data = []\n",
        "        for link in results['linked_entities']['wikidata_links']:\n",
        "            linked_data.append({\n",
        "                'Original_Entity': link['original_entity'],\n",
        "                'Linked_Label': link['label'],\n",
        "                'Description': link['description'],\n",
        "                'URI': link['uri'],\n",
        "                'Match_Score': link['match_score'],\n",
        "                'Source': link['source']\n",
        "            })\n",
        "\n",
        "        linked_df = pd.DataFrame(linked_data)\n",
        "\n",
        "        return entities_df, linked_df"
      ],
      "metadata": {
        "id": "0xcPMgUIpIDn"
      },
      "id": "0xcPMgUIpIDn",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# STEP 5: DEMO AND USAGE\n",
        "# ================================\n",
        "\n",
        "def demo_with_sample_resumes():\n",
        "    \"\"\"Demonstrate the resume parser with sample resumes\"\"\"\n",
        "\n",
        "    # Sample resume texts\n",
        "    sample_resumes = [\n",
        "        \"\"\"\n",
        "        John Smith\n",
        "        Data Scientist\n",
        "        Email: john.smith@email.com\n",
        "        Phone: (555) 123-4567\n",
        "        LinkedIn: linkedin.com/in/johnsmith\n",
        "\n",
        "        EDUCATION:\n",
        "        Master of Science in Computer Science, Stanford University (2020)\n",
        "        Bachelor of Science in Mathematics, MIT (2018)\n",
        "\n",
        "        EXPERIENCE:\n",
        "        Senior Data Scientist at Google (2021-2023)\n",
        "        Machine Learning Engineer at Facebook (2020-2021)\n",
        "\n",
        "        SKILLS:\n",
        "        Python, R, SQL, Machine Learning, Deep Learning, TensorFlow, PyTorch,\n",
        "        Natural Language Processing, Computer Vision, AWS, Docker, Kubernetes\n",
        "        \"\"\",\n",
        "\n",
        "        \"\"\"\n",
        "        Sarah Johnson\n",
        "        Software Engineer\n",
        "        sarah.johnson@tech.com\n",
        "        +1-555-987-6543\n",
        "\n",
        "        EDUCATION:\n",
        "        B.Tech in Computer Science and Engineering, IIT Delhi (2019)\n",
        "\n",
        "        PROFESSIONAL EXPERIENCE:\n",
        "        Software Development Engineer II at Amazon (2022-Present)\n",
        "        Full Stack Developer at Microsoft (2019-2022)\n",
        "\n",
        "        TECHNICAL SKILLS:\n",
        "        Java, JavaScript, React, Node.js, Spring Boot, MongoDB,\n",
        "        PostgreSQL, Git, Jenkins, Docker, Azure, Agile Methodology\n",
        "        \"\"\"\n",
        "    ]\n",
        "\n",
        "    # Initialize parser\n",
        "    print(\"üöÄ Initializing Resume Parser...\")\n",
        "    parser = ResumeParser()\n",
        "\n",
        "    for i, resume_text in enumerate(sample_resumes, 1):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"üìÑ PROCESSING RESUME {i}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Parse resume\n",
        "        results = parser.parse_resume(text=resume_text)\n",
        "\n",
        "        # Display summary\n",
        "        print(\"\\nüìä PARSING SUMMARY:\")\n",
        "        for key, value in results['summary'].items():\n",
        "            print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "        # Display entities\n",
        "        print(\"\\nüè∑Ô∏è  EXTRACTED ENTITIES:\")\n",
        "        entities_df, linked_df = parser.export_to_dataframe(results)\n",
        "        print(entities_df.to_string(index=False))\n",
        "\n",
        "        # Display top linked entities\n",
        "        print(\"\\nüîó TOP LINKED ENTITIES:\")\n",
        "        if not linked_df.empty:\n",
        "            top_links = linked_df.sort_values('Match_Score', ascending=False).head(5)\n",
        "            print(top_links[['Original_Entity', 'Linked_Label', 'Match_Score']].to_string(index=False))\n",
        "        else:\n",
        "            print(\"  No entities were successfully linked.\")\n",
        "\n",
        "        # Display contact info\n",
        "        if results['extracted_entities']['contact_info']:\n",
        "            print(\"\\nüìû CONTACT INFORMATION:\")\n",
        "            for key, value in results['extracted_entities']['contact_info'].items():\n",
        "                print(f\"  {key.title()}: {value}\")"
      ],
      "metadata": {
        "id": "aDFNx5xNpMUA"
      },
      "id": "aDFNx5xNpMUA",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# MAIN EXECUTION\n",
        "# ================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üìÑ Resume Parser with NER & Entity Linking\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Run demo\n",
        "    demo_with_sample_resumes()\n",
        "\n",
        "    # Example of parsing from file\n",
        "    print(\"\\n\\nüîß USAGE EXAMPLES:\")\n",
        "    print(\"# Parse from text file:\")\n",
        "    print(\"parser = ResumeParser()\")\n",
        "    print(\"results = parser.parse_resume(file_path='resume.txt')\")\n",
        "\n",
        "    print(\"\\n# Parse from PDF file:\")\n",
        "    print(\"results = parser.parse_resume(file_path='resume.pdf')\")\n",
        "\n",
        "    print(\"\\n# Parse from text string:\")\n",
        "    print(\"results = parser.parse_resume(text=your_resume_text)\")\n",
        "\n",
        "    print(\"\\n# Export to DataFrames:\")\n",
        "    print(\"entities_df, linked_df = parser.export_to_dataframe(results)\")\n",
        "    print(\"entities_df.to_csv('entities.csv', index=False)\")\n",
        "    print(\"linked_df.to_csv('linked_entities.csv', index=False)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53GxIDggpzN5",
        "outputId": "3a75653f-64fb-4dd8-c4da-7e01f8a8106f"
      },
      "id": "53GxIDggpzN5",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Resume Parser with NER & Entity Linking\n",
            "==================================================\n",
            "üöÄ Initializing Resume Parser...\n",
            "\n",
            "==================================================\n",
            "üìÑ PROCESSING RESUME 1\n",
            "==================================================\n",
            "\n",
            "üìä PARSING SUMMARY:\n",
            "  Total Entities: 15\n",
            "  Skills Found: 9\n",
            "  Education Entries: 1\n",
            "  Contact Fields: 1\n",
            "  Wikidata Links: 13\n",
            "  Dbpedia Links: 0\n",
            "\n",
            "üè∑Ô∏è  EXTRACTED ENTITIES:\n",
            "                   Entity   Type                            Description  Confidence\n",
            "               John Smith PERSON                           Person names         1.0\n",
            "      Stanford University    ORG Organizations, companies, universities         1.0\n",
            "                      MIT    ORG Organizations, companies, universities         1.0\n",
            "                   Google    ORG Organizations, companies, universities         1.0\n",
            "                2021-2023   DATE                           Dates, years         1.0\n",
            "Machine Learning Engineer    ORG Organizations, companies, universities         1.0\n",
            "                2020-2021   DATE                           Dates, years         1.0\n",
            "                      SQL    ORG Organizations, companies, universities         1.0\n",
            "         Machine Learning PERSON                           Person names         1.0\n",
            "            Deep Learning PERSON                           Person names         1.0\n",
            "               TensorFlow    ORG Organizations, companies, universities         1.0\n",
            "                  PyTorch    ORG Organizations, companies, universities         1.0\n",
            "                      AWS    ORG Organizations, companies, universities         1.0\n",
            "                   Docker PERSON                           Person names         1.0\n",
            "               Kubernetes    ORG Organizations, companies, universities         1.0\n",
            "                   Python  SKILL                  Category: programming         0.9\n",
            "                       Go  SKILL                  Category: programming         0.9\n",
            "         Machine Learning  SKILL                        Category: ml_ai         0.9\n",
            "            Deep Learning  SKILL                        Category: ml_ai         0.9\n",
            "               Tensorflow  SKILL                        Category: ml_ai         0.9\n",
            "                  Pytorch  SKILL                        Category: ml_ai         0.9\n",
            "                      Aws  SKILL                        Category: cloud         0.9\n",
            "                   Docker  SKILL                        Category: cloud         0.9\n",
            "               Kubernetes  SKILL                        Category: cloud         0.9\n",
            "\n",
            "üîó TOP LINKED ENTITIES:\n",
            "Original_Entity Linked_Label  Match_Score\n",
            "         Docker       Docker          1.0\n",
            "     Kubernetes   Kubernetes          1.0\n",
            "         Python       Python          1.0\n",
            "         Python       Python          1.0\n",
            "         Google       Google          1.0\n",
            "\n",
            "üìû CONTACT INFORMATION:\n",
            "  Phone: (555) 123-4567\n",
            "\n",
            "==================================================\n",
            "üìÑ PROCESSING RESUME 2\n",
            "==================================================\n",
            "\n",
            "üìä PARSING SUMMARY:\n",
            "  Total Entities: 16\n",
            "  Skills Found: 11\n",
            "  Education Entries: 1\n",
            "  Contact Fields: 1\n",
            "  Wikidata Links: 12\n",
            "  Dbpedia Links: 0\n",
            "\n",
            "üè∑Ô∏è  EXTRACTED ENTITIES:\n",
            "                          Entity        Type                            Description  Confidence\n",
            "          Sarah Johnson Software      PERSON                           Person names         1.0\n",
            "                          B.Tech         ORG Organizations, companies, universities         1.0\n",
            "Computer Science and Engineering         ORG Organizations, companies, universities         1.0\n",
            "                       IIT Delhi         GPE              Countries, cities, states         1.0\n",
            "                          Amazon         ORG Organizations, companies, universities         1.0\n",
            "                    2022-Present       MONEY            Salaries, financial amounts         1.0\n",
            "                       Microsoft         ORG Organizations, companies, universities         1.0\n",
            "                       2019-2022        DATE                           Dates, years         1.0\n",
            "                            Java      PERSON                           Person names         1.0\n",
            "                      JavaScript         ORG Organizations, companies, universities         1.0\n",
            "                           React         GPE              Countries, cities, states         1.0\n",
            "                         Node.js         GPE              Countries, cities, states         1.0\n",
            "            Spring Boot, MongoDB WORK_OF_ART                 Publications, projects         1.0\n",
            "                      PostgreSQL         GPE              Countries, cities, states         1.0\n",
            "                   Git, Jenkins,         GPE              Countries, cities, states         1.0\n",
            "                          Docker      PERSON                           Person names         1.0\n",
            "                            Java       SKILL                  Category: programming         0.9\n",
            "                      Javascript       SKILL                  Category: programming         0.9\n",
            "                              Go       SKILL                  Category: programming         0.9\n",
            "                      Postgresql       SKILL                    Category: databases         0.9\n",
            "                         Mongodb       SKILL                    Category: databases         0.9\n",
            "                           React       SKILL                   Category: frameworks         0.9\n",
            "                          Spring       SKILL                   Category: frameworks         0.9\n",
            "                           Azure       SKILL                        Category: cloud         0.9\n",
            "                          Docker       SKILL                        Category: cloud         0.9\n",
            "                             Git       SKILL                        Category: tools         0.9\n",
            "                         Jenkins       SKILL                        Category: tools         0.9\n",
            "\n",
            "üîó TOP LINKED ENTITIES:\n",
            "Original_Entity Linked_Label  Match_Score\n",
            "         Docker       Docker          1.0\n",
            "        Node.js      Node.js          1.0\n",
            "         Amazon       Amazon          1.0\n",
            "     PostgreSQL   PostgreSQL          1.0\n",
            "         Amazon       Amazon          1.0\n",
            "\n",
            "üìû CONTACT INFORMATION:\n",
            "  Phone: 555-987-6543\n",
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
